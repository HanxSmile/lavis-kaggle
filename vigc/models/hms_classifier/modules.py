import torch.nn as nn
import torch
import torch.nn.functional as F
import numpy as np


class GeM(nn.Module):
    def __init__(self, p=3, eps=1e-6):
        super(GeM, self).__init__()
        self.p = nn.Parameter(torch.ones(1) * p)
        self.eps = eps

    def forward(self, x):
        res = self.gem(x, p=self.p, eps=self.eps)
        return res.squeeze(-1).squeeze(-1)

    def gem(self, x, p=3, eps=1e-6):
        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)

    def __repr__(self):
        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(
            self.eps) + ')'


class KLDivLossWithLogits(nn.KLDivLoss):

    def __init__(self, reduction="batchmean"):
        super().__init__(reduction=reduction)

    def forward(self, y, t):
        y = nn.functional.log_softmax(y, dim=1)
        loss = super().forward(y, t)

        return loss


class KLDivLossWithLogitsForVal(nn.KLDivLoss):

    def __init__(self):
        """"""
        super().__init__(reduction="batchmean")
        self.log_prob_list = []
        self.label_list = []

    def forward(self, y, t):
        y = nn.functional.log_softmax(y, dim=1)
        self.log_prob_list.append(y.numpy())
        self.label_list.append(t.numpy())

    def compute(self):
        log_prob = np.concatenate(self.log_prob_list, axis=0)
        label = np.concatenate(self.label_list, axis=0)
        final_metric = super().forward(
            torch.from_numpy(log_prob),
            torch.from_numpy(label)
        ).item()
        self.log_prob_list = []
        self.label_list = []

        return final_metric


if __name__ == '__main__':
    pred = torch.randn(2, 6)
    pred = nn.functional.log_softmax(pred, dim=1)
    target = F.softmax(torch.randn(2, 6), dim=1)
    fn = nn.KLDivLoss(reduction="none")
    print(fn(pred, target).shape)
    # print(y.shape)
